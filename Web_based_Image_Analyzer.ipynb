{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Web_based_Image_Analyzer.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Tathagatd96/Web-based-Image-Analyzer/blob/master/Web_based_Image_Analyzer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-hyrFtlDY_L",
        "colab_type": "text"
      },
      "source": [
        "**Data Fetching and Preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9P743KBai6HX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import os\n",
        "def readdir(path):\n",
        "  for files in os.listdir(path):\n",
        "    if os.path.isdir(os.path.join(path,files)):\n",
        "      print('Directory -> {}\\n'.format(files))\n",
        "    else:\n",
        "      print('File -> {}\\n'.format(files))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "elqi7jpgjBPl",
        "colab_type": "code",
        "outputId": "0c07f57a-36d0-40f6-b773-6011af938795",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "!git clone https://github.com/EscVM/OIDv4_ToolKit.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'OIDv4_ToolKit'...\n",
            "remote: Enumerating objects: 23, done.\u001b[K\n",
            "remote: Counting objects: 100% (23/23), done.\u001b[K\n",
            "remote: Compressing objects: 100% (21/21), done.\u001b[K\n",
            "remote: Total 422 (delta 3), reused 12 (delta 2), pack-reused 399\u001b[K\n",
            "Receiving objects: 100% (422/422), 34.10 MiB | 27.87 MiB/s, done.\n",
            "Resolving deltas: 100% (132/132), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4wao8ccsjEX3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil\n",
        "path = os.getcwd() + \"/OIDv4_ToolKit\"\n",
        "excp = ['LICENSE','README.md','.gitignore','.git','classes.txt','requirements.txt']\n",
        "for files in os.listdir(path):\n",
        "  if not os.path.isdir(os.path.join(path,files)):\n",
        "    if files not in excp:\n",
        "      shutil.copy(os.path.join(path,files), os.getcwd())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yUWLxZWxjHlA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir(os.getcwd() + \"/modules\")\n",
        "path = os.getcwd() + \"/OIDv4_ToolKit/modules\"\n",
        "for files in os.listdir(path):\n",
        "  shutil.copy(os.path.join(path,files), os.getcwd() + \"/modules\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaHZlMMEjJ4M",
        "colab_type": "code",
        "outputId": "9a8e8e2f-ea69-4030-9366-1619567f0236",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!pip install awscli\n",
        "!python3 main.py downloader --classes Apple Banana Broccoli Cabbage Carrot Grape Mango Orange Pear Pineapple Potato Radish Squash Strawberry Watermelon Zucchini --type_csv train"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting awscli\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/78/6ed315818152c41b2dcd34b60bbd06bd0d0994dc48ff98f12b77d49d5334/awscli-1.16.285-py2.py3-none-any.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 4.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: s3transfer<0.3.0,>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from awscli) (0.2.1)\n",
            "Collecting botocore==1.13.21\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/cd/28982baea070aaf2f372de89b9e76899b7d362f9874ee8b12090d8c337ac/botocore-1.13.21-py2.py3-none-any.whl (5.5MB)\n",
            "\u001b[K     |████████████████████████████████| 5.5MB 51.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML<5.2,>=3.10; python_version != \"2.6\" and python_version != \"3.3\" in /usr/local/lib/python3.6/dist-packages (from awscli) (3.13)\n",
            "Collecting rsa<=3.5.0,>=3.1.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e1/ae/baedc9cb175552e95f3395c43055a6a5e125ae4d48a1d7a924baca83e92e/rsa-3.4.2-py2.py3-none-any.whl (46kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 7.6MB/s \n",
            "\u001b[?25hCollecting colorama<0.4.2,>=0.2.5; python_version != \"2.6\" and python_version != \"3.3\"\n",
            "  Downloading https://files.pythonhosted.org/packages/4f/a6/728666f39bfff1719fc94c481890b2106837da9318031f71a8424b662e12/colorama-0.4.1-py2.py3-none-any.whl\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from awscli) (0.15.2)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from botocore==1.13.21->awscli) (0.9.4)\n",
            "Requirement already satisfied: python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore==1.13.21->awscli) (2.6.1)\n",
            "Requirement already satisfied: urllib3<1.26,>=1.20; python_version >= \"3.4\" in /usr/local/lib/python3.6/dist-packages (from botocore==1.13.21->awscli) (1.24.3)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=3.5.0,>=3.1.2->awscli) (0.4.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil<2.8.1,>=2.1; python_version >= \"2.7\"->botocore==1.13.21->awscli) (1.12.0)\n",
            "Installing collected packages: botocore, rsa, colorama, awscli\n",
            "  Found existing installation: botocore 1.13.18\n",
            "    Uninstalling botocore-1.13.18:\n",
            "      Successfully uninstalled botocore-1.13.18\n",
            "  Found existing installation: rsa 4.0\n",
            "    Uninstalling rsa-4.0:\n",
            "      Successfully uninstalled rsa-4.0\n",
            "Successfully installed awscli-1.16.285 botocore-1.13.21 colorama-0.4.1 rsa-3.4.2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "rsa"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\u001b[92m\n",
            "\t\t   ___   _____  ______            _    _    \n",
            "\t\t .'   `.|_   _||_   _ `.         | |  | |   \n",
            "\t\t/  .-.  \\ | |    | | `. \\ _   __ | |__| |_  \n",
            "\t\t| |   | | | |    | |  | |[ \\ [  ]|____   _| \n",
            "\t\t\\  `-'  /_| |_  _| |_.' / \\ \\/ /     _| |_  \n",
            "\t\t `.___.'|_____||______.'   \\__/     |_____|\n",
            "\t\u001b[0m\n",
            "\u001b[92m\n",
            "             _____                    _                 _             \n",
            "            (____ \\                  | |               | |            \n",
            "             _   \\ \\ ___  _ _ _ ____ | | ___   ____  _ | | ____  ____ \n",
            "            | |   | / _ \\| | | |  _ \\| |/ _ \\ / _  |/ || |/ _  )/ ___)\n",
            "            | |__/ / |_| | | | | | | | | |_| ( ( | ( (_| ( (/ /| |    \n",
            "            |_____/ \\___/ \\____|_| |_|_|\\___/ \\_||_|\\____|\\____)_|    \n",
            "                                                          \n",
            "        \u001b[0m\n",
            "    [INFO] | Downloading Apple.\u001b[0m\n",
            "\u001b[91m   [ERROR] | Missing the class-descriptions-boxable.csv file.\u001b[0m\n",
            "\u001b[94m[DOWNLOAD] | Do you want to download the missing file? [Y/n] \u001b[0my\n",
            "...145%, 0 MB, 52427 KB/s, 0 seconds passed\n",
            "\u001b[94m[DOWNLOAD] | File class-descriptions-boxable.csv downloaded into OID/csv_folder/class-descriptions-boxable.csv.\u001b[0m\n",
            "\u001b[91m   [ERROR] | Missing the train-annotations-bbox.csv file.\u001b[0m\n",
            "\u001b[94m[DOWNLOAD] | Do you want to download the missing file? [Y/n] \u001b[0my\n",
            "...100%, 1138 MB, 52339 KB/s, 22 seconds passed\n",
            "\u001b[94m[DOWNLOAD] | File train-annotations-bbox.csv downloaded into OID/csv_folder/train-annotations-bbox.csv.\u001b[0m\n",
            "\n",
            "\u001b[95mApple\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 1078 online images for train.\u001b[0m\n",
            "    [INFO] | Download of 1078 images in train.\u001b[0m\n",
            "100% 1078/1078 [07:46<00:00,  3.31it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Apple of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Banana.\u001b[0m\n",
            "\n",
            "\u001b[95mBanana\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 723 online images for train.\u001b[0m\n",
            "    [INFO] | Download of 723 images in train.\u001b[0m\n",
            "100% 723/723 [05:10<00:00,  2.33it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Banana of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Broccoli.\u001b[0m\n",
            "\n",
            "\u001b[95mBroccoli\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 475 online images for train.\u001b[0m\n",
            "    [INFO] | Download of 475 images in train.\u001b[0m\n",
            "100% 475/475 [03:32<00:00,  2.23it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Broccoli of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Cabbage.\u001b[0m\n",
            "\n",
            "\u001b[95mCabbage\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 276 online images for train.\u001b[0m\n",
            "    [INFO] | Download of 276 images in train.\u001b[0m\n",
            "100% 276/276 [02:06<00:00,  4.26it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Cabbage of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Carrot.\u001b[0m\n",
            "\n",
            "\u001b[95mCarrot\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 594 online images for train.\u001b[0m\n",
            "    [INFO] | Download of 594 images in train.\u001b[0m\n",
            "100% 594/594 [04:31<00:00,  2.19it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Carrot of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Grape.\u001b[0m\n",
            "\n",
            "\u001b[95mGrape\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 767 online images for train.\u001b[0m\n",
            "    [INFO] | Download of 767 images in train.\u001b[0m\n",
            "100% 767/767 [05:56<00:00,  2.15it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Grape of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Mango.\u001b[0m\n",
            "\n",
            "\u001b[95mMango\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 126 online images for train.\u001b[0m\n",
            "    [INFO] | Download of 126 images in train.\u001b[0m\n",
            "100% 126/126 [00:59<00:00,  2.11it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Mango of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Orange.\u001b[0m\n",
            "\n",
            "\u001b[95mOrange\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 900 online images for train.\u001b[0m\n",
            "    [INFO] | Download of 900 images in train.\u001b[0m\n",
            "100% 900/900 [07:05<00:00,  2.11it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Orange of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Pear.\u001b[0m\n",
            "\n",
            "\u001b[95mPear\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 263 online images for train.\u001b[0m\n",
            "    [INFO] | Download of 263 images in train.\u001b[0m\n",
            "100% 263/263 [02:06<00:00,  2.08it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Pear of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Pineapple.\u001b[0m\n",
            "\n",
            "\u001b[95mPineapple\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 310 online images for train.\u001b[0m\n",
            "    [INFO] | Download of 310 images in train.\u001b[0m\n",
            "100% 310/310 [02:26<00:00,  4.19it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Pineapple of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Potato.\u001b[0m\n",
            "\n",
            "\u001b[95mPotato\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 304 online images for train.\u001b[0m\n",
            "    [INFO] | Download of 304 images in train.\u001b[0m\n",
            "100% 304/304 [02:19<00:00,  2.17it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Potato of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Radish.\u001b[0m\n",
            "\n",
            "\u001b[95mRadish\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 264 online images for train.\u001b[0m\n",
            "    [INFO] | Download of 264 images in train.\u001b[0m\n",
            "100% 264/264 [02:11<00:00,  2.01it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Radish of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Squash.\u001b[0m\n",
            "\n",
            "\u001b[95mSquash\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 122 online images for train.\u001b[0m\n",
            "    [INFO] | Download of 122 images in train.\u001b[0m\n",
            "100% 122/122 [00:55<00:00,  2.21it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Squash of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Strawberry.\u001b[0m\n",
            "\n",
            "\u001b[95mStrawberry\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 1257 online images for train.\u001b[0m\n",
            "    [INFO] | Download of 1257 images in train.\u001b[0m\n",
            "100% 1257/1257 [10:07<00:00,  2.07it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Strawberry of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Watermelon.\u001b[0m\n",
            "\n",
            "\u001b[95mWatermelon\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 409 online images for train.\u001b[0m\n",
            "    [INFO] | Download of 409 images in train.\u001b[0m\n",
            "100% 409/409 [03:23<00:00,  2.01it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Watermelon of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Zucchini.\u001b[0m\n",
            "\n",
            "\u001b[95mZucchini\u001b[0m\n",
            "    [INFO] | Downloading train images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 287 online images for train.\u001b[0m\n",
            "    [INFO] | Download of 287 images in train.\u001b[0m\n",
            "100% 287/287 [02:18<00:00,  3.49it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Zucchini of train.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRLr3E7f0noo",
        "colab_type": "code",
        "outputId": "02e1736e-9476-494a-9691-253cfc3e8cd2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!python3 main.py downloader --classes Apple Banana Broccoli Cabbage Carrot Grape Mango Orange Pear Pineapple Potato Radish Squash Strawberry Watermelon Zucchini --type_csv validation"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[92m\n",
            "\t\t   ___   _____  ______            _    _    \n",
            "\t\t .'   `.|_   _||_   _ `.         | |  | |   \n",
            "\t\t/  .-.  \\ | |    | | `. \\ _   __ | |__| |_  \n",
            "\t\t| |   | | | |    | |  | |[ \\ [  ]|____   _| \n",
            "\t\t\\  `-'  /_| |_  _| |_.' / \\ \\/ /     _| |_  \n",
            "\t\t `.___.'|_____||______.'   \\__/     |_____|\n",
            "\t\u001b[0m\n",
            "\u001b[92m\n",
            "             _____                    _                 _             \n",
            "            (____ \\                  | |               | |            \n",
            "             _   \\ \\ ___  _ _ _ ____ | | ___   ____  _ | | ____  ____ \n",
            "            | |   | / _ \\| | | |  _ \\| |/ _ \\ / _  |/ || |/ _  )/ ___)\n",
            "            | |__/ / |_| | | | | | | | | |_| ( ( | ( (_| ( (/ /| |    \n",
            "            |_____/ \\___/ \\____|_| |_|_|\\___/ \\_||_|\\____|\\____)_|    \n",
            "                                                          \n",
            "        \u001b[0m\n",
            "    [INFO] | Downloading Apple.\u001b[0m\n",
            "\u001b[91m   [ERROR] | Missing the validation-annotations-bbox.csv file.\u001b[0m\n",
            "\u001b[94m[DOWNLOAD] | Do you want to download the missing file? [Y/n] \u001b[0my\n",
            "...100%, 16 MB, 43247 KB/s, 0 seconds passed\n",
            "\u001b[94m[DOWNLOAD] | File validation-annotations-bbox.csv downloaded into OID/csv_folder/validation-annotations-bbox.csv.\u001b[0m\n",
            "\n",
            "\u001b[95mApple\u001b[0m\n",
            "    [INFO] | Downloading validation images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 46 online images for validation.\u001b[0m\n",
            "    [INFO] | Download of 46 images in validation.\u001b[0m\n",
            "100% 46/46 [00:26<00:00,  1.67it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Apple of validation.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Banana.\u001b[0m\n",
            "\n",
            "\u001b[95mBanana\u001b[0m\n",
            "    [INFO] | Downloading validation images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 17 online images for validation.\u001b[0m\n",
            "    [INFO] | Download of 17 images in validation.\u001b[0m\n",
            "100% 17/17 [00:09<00:00,  1.75it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Banana of validation.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Broccoli.\u001b[0m\n",
            "\n",
            "\u001b[95mBroccoli\u001b[0m\n",
            "    [INFO] | Downloading validation images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 18 online images for validation.\u001b[0m\n",
            "    [INFO] | Download of 18 images in validation.\u001b[0m\n",
            "100% 18/18 [00:10<00:00,  1.73it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Broccoli of validation.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Cabbage.\u001b[0m\n",
            "\n",
            "\u001b[95mCabbage\u001b[0m\n",
            "    [INFO] | Downloading validation images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 22 online images for validation.\u001b[0m\n",
            "    [INFO] | Download of 22 images in validation.\u001b[0m\n",
            "100% 22/22 [00:15<00:00,  3.96s/it]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Cabbage of validation.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Carrot.\u001b[0m\n",
            "\n",
            "\u001b[95mCarrot\u001b[0m\n",
            "    [INFO] | Downloading validation images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 32 online images for validation.\u001b[0m\n",
            "    [INFO] | Download of 32 images in validation.\u001b[0m\n",
            "100% 32/32 [00:18<00:00,  1.72it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Carrot of validation.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Grape.\u001b[0m\n",
            "\n",
            "\u001b[95mGrape\u001b[0m\n",
            "    [INFO] | Downloading validation images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 44 online images for validation.\u001b[0m\n",
            "    [INFO] | Download of 44 images in validation.\u001b[0m\n",
            "100% 44/44 [00:26<00:00,  1.69it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Grape of validation.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Mango.\u001b[0m\n",
            "\n",
            "\u001b[95mMango\u001b[0m\n",
            "    [INFO] | Downloading validation images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 4 online images for validation.\u001b[0m\n",
            "    [INFO] | Download of 4 images in validation.\u001b[0m\n",
            "100% 4/4 [00:05<00:00,  1.25s/it]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Mango of validation.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Orange.\u001b[0m\n",
            "\n",
            "\u001b[95mOrange\u001b[0m\n",
            "    [INFO] | Downloading validation images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 61 online images for validation.\u001b[0m\n",
            "    [INFO] | Download of 61 images in validation.\u001b[0m\n",
            "100% 61/61 [00:32<00:00,  1.90it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Orange of validation.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Pear.\u001b[0m\n",
            "\n",
            "\u001b[95mPear\u001b[0m\n",
            "    [INFO] | Downloading validation images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 13 online images for validation.\u001b[0m\n",
            "    [INFO] | Download of 13 images in validation.\u001b[0m\n",
            "100% 13/13 [00:07<00:00,  1.64it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Pear of validation.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Pineapple.\u001b[0m\n",
            "\n",
            "\u001b[95mPineapple\u001b[0m\n",
            "    [INFO] | Downloading validation images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 21 online images for validation.\u001b[0m\n",
            "    [INFO] | Download of 21 images in validation.\u001b[0m\n",
            "100% 21/21 [00:15<00:00,  5.48s/it]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Pineapple of validation.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Potato.\u001b[0m\n",
            "\n",
            "\u001b[95mPotato\u001b[0m\n",
            "    [INFO] | Downloading validation images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 28 online images for validation.\u001b[0m\n",
            "    [INFO] | Download of 28 images in validation.\u001b[0m\n",
            "100% 28/28 [00:16<00:00,  1.66it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Potato of validation.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Radish.\u001b[0m\n",
            "\n",
            "\u001b[95mRadish\u001b[0m\n",
            "    [INFO] | Downloading validation images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 17 online images for validation.\u001b[0m\n",
            "    [INFO] | Download of 17 images in validation.\u001b[0m\n",
            "100% 17/17 [00:09<00:00,  1.74it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Radish of validation.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Squash.\u001b[0m\n",
            "\n",
            "\u001b[95mSquash\u001b[0m\n",
            "    [INFO] | Downloading validation images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 16 online images for validation.\u001b[0m\n",
            "    [INFO] | Download of 16 images in validation.\u001b[0m\n",
            "100% 16/16 [00:09<00:00,  1.69it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Squash of validation.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Strawberry.\u001b[0m\n",
            "\n",
            "\u001b[95mStrawberry\u001b[0m\n",
            "    [INFO] | Downloading validation images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 63 online images for validation.\u001b[0m\n",
            "    [INFO] | Download of 63 images in validation.\u001b[0m\n",
            "100% 63/63 [00:32<00:00,  2.09it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Strawberry of validation.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Watermelon.\u001b[0m\n",
            "\n",
            "\u001b[95mWatermelon\u001b[0m\n",
            "    [INFO] | Downloading validation images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 24 online images for validation.\u001b[0m\n",
            "    [INFO] | Download of 24 images in validation.\u001b[0m\n",
            "100% 24/24 [00:16<00:00,  1.49it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Watermelon of validation.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n",
            "    [INFO] | Downloading Zucchini.\u001b[0m\n",
            "\n",
            "\u001b[95mZucchini\u001b[0m\n",
            "    [INFO] | Downloading validation images.\u001b[0m\n",
            "    [INFO] | [INFO] Found 32 online images for validation.\u001b[0m\n",
            "    [INFO] | Download of 32 images in validation.\u001b[0m\n",
            "100% 32/32 [00:18<00:00,  1.74it/s]\n",
            "    [INFO] | Done!\u001b[0m\n",
            "    [INFO] | Creating labels for Zucchini of validation.\u001b[0m\n",
            "    [INFO] | Labels creation completed.\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7E_2kMF2MQX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.remove('main.py')\n",
        "shutil.rmtree('modules')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KY5mfZte2Wzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset')\n",
        "os.mkdir('Dataset/train')\n",
        "os.mkdir('Dataset/validation')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VlpYk_br2aB9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = os.getcwd() + \"/OID/Dataset/train\"\n",
        "newpath = os.getcwd() + \"/Dataset/train\"\n",
        "for classes in os.listdir(path):\n",
        "  os.mkdir(os.path.join(newpath,classes))\n",
        "  os.mkdir(os.path.join(newpath,classes,\"label\"))\n",
        "  counter = 0\n",
        "  it = 0\n",
        "  files = os.listdir(os.path.join(path,classes))\n",
        "  file = os.path.join(path,classes,files[it])\n",
        "  while counter < 80:\n",
        "    if os.path.isdir(file):\n",
        "      it += 1\n",
        "    else:\n",
        "      shutil.copy(file,os.path.join(newpath,classes))\n",
        "      shutil.copy(path + \"/\" + classes + \"/Label/\" + files[it].split(\".\")[0] + \".txt\",os.path.join(newpath,classes,\"label\"))\n",
        "      it += 1\n",
        "      counter += 1\n",
        "\n",
        "    file = os.path.join(path,classes,files[it])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqdoOR6W4c4e",
        "colab_type": "text"
      },
      "source": [
        "**Convert .txt annotations to .xml format**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-bv0gFW4It4",
        "colab_type": "code",
        "outputId": "7c7b7c9c-b3e9-4a1a-b999-f112803287c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        }
      },
      "source": [
        "!git clone https://github.com/AtriSaxena/OIDv4_to_VOC.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'OIDv4_to_VOC'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "remote: Total 14 (delta 0), reused 0 (delta 0), pack-reused 14\u001b[K\n",
            "Unpacking objects: 100% (14/14), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CONukKvP4h9h",
        "colab_type": "code",
        "outputId": "c2d05d8a-07ce-4a4e-e09f-1cad01291008",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "path = os.getcwd() + \"/OIDv4_to_VOC/OIDv4_to_VOC.py\"\n",
        "shutil.copy(path,os.getcwd())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/OIDv4_to_VOC.py'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O54HFTb14okR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/train/Annotations')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yy1DoOTE4xxa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/train/Annotations/Apple')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath Dataset/train/Apple --dest_path Dataset/train/Annotations/Apple"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wF4_GvZl5HeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/train/Annotations/Banana')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath Dataset/train/Banana --dest_path Dataset/train/Annotations/Banana"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iwE5EBMT5Rzf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/train/Annotations/Broccoli')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath Dataset/train/Broccoli --dest_path Dataset/train/Annotations/Broccoli"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_maSZFIv5ZRI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/train/Annotations/Cabbage')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath Dataset/train/Cabbage --dest_path Dataset/train/Annotations/Cabbage"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R79KWRD05hL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/train/Annotations/Carrot')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath Dataset/train/Carrot --dest_path Dataset/train/Annotations/Carrot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pn_mGXAk6AUU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/train/Annotations/Grape')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath Dataset/train/Grape --dest_path Dataset/train/Annotations/Grape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic5Qczhp6HUO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#shutil.rmtree('Dataset/train/Annotations/Mango')\n",
        "os.mkdir('Dataset/train/Annotations/Mango')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath Dataset/train/Mango --dest_path Dataset/train/Annotations/Mango"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ti28WQNB6Uqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/train/Annotations/Orange')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath Dataset/train/Orange --dest_path Dataset/train/Annotations/Orange"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VHba7A8H6bN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/train/Annotations/Pear')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath Dataset/train/Pear --dest_path Dataset/train/Annotations/Pear"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o4QxfDsx6hdY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/train/Annotations/Pineapple')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath Dataset/train/Pineapple --dest_path Dataset/train/Annotations/Pineapple"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlM4SpkS6nRo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/train/Annotations/Potato')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath Dataset/train/Potato --dest_path Dataset/train/Annotations/Potato"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yddNQmQ96tV_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#shutil.rmtree('Dataset/train/Annotations/Radish')\n",
        "os.mkdir('Dataset/train/Annotations/Radish')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath Dataset/train/Radish --dest_path Dataset/train/Annotations/Radish"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a9WbYI6o65Be",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/train/Annotations/Squash')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath Dataset/train/Squash --dest_path Dataset/train/Annotations/Squash"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCsr1BWO6_K5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/train/Annotations/Strawberry')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath Dataset/train/Strawberry --dest_path Dataset/train/Annotations/Strawberry"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tQT6HR2C7GMo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/train/Annotations/Watermelon')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath Dataset/train/Watermelon --dest_path Dataset/train/Annotations/Watermelon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnZq3dRu7SPV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/train/Annotations/Zucchini')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath Dataset/train/Zucchini --dest_path Dataset/train/Annotations/Zucchini"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PBoVZF198m_M",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/validation/Annotations')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_XOM0v_8yuH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/validation/Annotations/Apple')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath OID/Dataset/validation/Apple --dest_path Dataset/validation/Annotations/Apple"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOWAvq5u_N3U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/validation/Annotations/Banana')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath OID/Dataset/validation/Banana --dest_path Dataset/validation/Annotations/Banana"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvzpN8js_VzX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/validation/Annotations/Broccoli')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath OID/Dataset/validation/Broccoli --dest_path Dataset/validation/Annotations/Broccoli"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfIb-ge-_a8i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/validation/Annotations/Cabbage')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath OID/Dataset/validation/Cabbage --dest_path Dataset/validation/Annotations/Cabbage"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rwCw3xqU_jsL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/validation/Annotations/Carrot')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath OID/Dataset/validation/Carrot --dest_path Dataset/validation/Annotations/Carrot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UfKnvqvI_o5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/validation/Annotations/Grape')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath OID/Dataset/validation/Grape --dest_path Dataset/validation/Annotations/Grape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jWrsIkZ4_vVw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/validation/Annotations/Mango')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath OID/Dataset/validation/Mango --dest_path Dataset/validation/Annotations/Mango"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jA0tn9mB_2g8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/validation/Annotations/Orange')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath OID/Dataset/validation/Orange --dest_path Dataset/validation/Annotations/Orange"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Z7VjkWZ_65y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/validation/Annotations/Pear')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath OID/Dataset/validation/Pear --dest_path Dataset/validation/Annotations/Pear"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EEGSCy0M__dO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/validation/Annotations/Pineapple')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath OID/Dataset/validation/Pineapple --dest_path Dataset/validation/Annotations/Pineapple"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvOmN4FuAEYG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/validation/Annotations/Potato')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath OID/Dataset/validation/Potato --dest_path Dataset/validation/Annotations/Potato"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8XKTaBzAN8H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/validation/Annotations/Radish')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath OID/Dataset/validation/Radish --dest_path Dataset/validation/Annotations/Radish"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxX4nAAnASG9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/validation/Annotations/Squash')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath OID/Dataset/validation/Squash --dest_path Dataset/validation/Annotations/Squash"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGVRR21iAWNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/validation/Annotations/Strawberry')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath OID/Dataset/validation/Strawberry --dest_path Dataset/validation/Annotations/Strawberry"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBETiTp-AbD3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/validation/Annotations/Watermelon')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath OID/Dataset/validation/Watermelon --dest_path Dataset/validation/Annotations/Watermelon"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yR0zkjo8Afc-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('Dataset/validation/Annotations/Zucchini')\n",
        "!python3 OIDv4_to_VOC.py --sourcepath OID/Dataset/validation/Zucchini --dest_path Dataset/validation/Annotations/Zucchini"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r87Ynt2_7_ON",
        "colab_type": "text"
      },
      "source": [
        "**Create final image and annot folder**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zf78Objf73B6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "os.mkdir('train_images')\n",
        "os.mkdir('val_images')\n",
        "os.mkdir('train_annots')\n",
        "os.mkdir('val_annots')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2_UGS3EApXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = 'Dataset/train'\n",
        "for classes in os.listdir(path):\n",
        "  if classes == 'Annotations':\n",
        "    continue\n",
        "  else:\n",
        "    for files in os.listdir(os.path.join(path,classes)):\n",
        "      if not os.path.isdir(os.path.join(path,classes,files)):\n",
        "        #print(os.path.join(path,classes,files))\n",
        "        shutil.copy(os.path.join(path,classes,files), 'train_images')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIB8ZkmRB2Pz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = 'Dataset/train/Annotations'\n",
        "for classes in os.listdir(path):\n",
        "    for files in os.listdir(os.path.join(path,classes)):\n",
        "      #print(os.path.join(path,classes,files))\n",
        "      shutil.copy(os.path.join(path,classes,files), 'train_annots')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBlxoM73CSoO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = 'OID/Dataset/validation'\n",
        "for classes in os.listdir(path):\n",
        "    for files in os.listdir(os.path.join(path,classes)):\n",
        "      if not os.path.isdir(os.path.join(path,classes,files)):\n",
        "        #print(os.path.join(path,classes,files))\n",
        "        shutil.copy(os.path.join(path,classes,files), 'val_images')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXKbvbfuCnrC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path = 'Dataset/validation/Annotations'\n",
        "for classes in os.listdir(path):\n",
        "    for files in os.listdir(os.path.join(path,classes)):\n",
        "      #print(os.path.join(path,classes,files))\n",
        "      shutil.copy(os.path.join(path,classes,files), 'val_annots')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2q9nH6nC60F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shutil.rmtree('OIDv4_to_VOC')\n",
        "os.remove('OIDv4_to_VOC.py')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-g_dDwGIDLMH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shutil.rmtree('OIDv4_ToolKit')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBWQXHepDgRL",
        "colab_type": "text"
      },
      "source": [
        "**Data Preprocessing & Training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gP5wanc5DkbJ",
        "colab_type": "code",
        "outputId": "76c71126-c17c-4cf9-c44c-a91851bca8cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "!git clone https://github.com/experiencor/keras-yolo2.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'keras-yolo2'...\n",
            "remote: Enumerating objects: 324, done.\u001b[K\n",
            "remote: Total 324 (delta 0), reused 0 (delta 0), pack-reused 324\u001b[K\n",
            "Receiving objects: 100% (324/324), 53.90 MiB | 31.32 MiB/s, done.\n",
            "Resolving deltas: 100% (179/179), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PLOSuN7-D6T4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "excp = ['LICENSE','requirements.txt','README.md','.git','Yolo Step-by-Step.ipynb']\n",
        "path = os.getcwd() + \"/keras-yolo2\"\n",
        "for files in os.listdir(path):\n",
        "  if not os.path.isdir(os.path.join(path,files)):\n",
        "    shutil.copy(os.path.join(path,files),os.getcwd())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VJeGNkGG0RX0",
        "colab_type": "code",
        "outputId": "40d72306-26f3-4d99-8d45-f80a1e3481d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        }
      },
      "source": [
        "path = os.getcwd()\n",
        "readdir(path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Directory -> .config\n",
            "\n",
            "File -> README.md\n",
            "\n",
            "File -> predict.py\n",
            "\n",
            "File -> gen_anchors.py\n",
            "\n",
            "Directory -> train_images\n",
            "\n",
            "File -> train.py\n",
            "\n",
            "Directory -> val_annots\n",
            "\n",
            "Directory -> train_annots\n",
            "\n",
            "File -> LICENSE\n",
            "\n",
            "File -> preprocessing.py\n",
            "\n",
            "File -> Yolo Step-by-Step.ipynb\n",
            "\n",
            "File -> utils.py\n",
            "\n",
            "Directory -> Dataset\n",
            "\n",
            "File -> backend.py\n",
            "\n",
            "File -> requirements.txt\n",
            "\n",
            "Directory -> val_images\n",
            "\n",
            "Directory -> keras-yolo2\n",
            "\n",
            "File -> frontend.py\n",
            "\n",
            "Directory -> OID\n",
            "\n",
            "File -> config.json\n",
            "\n",
            "Directory -> sample_data\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiOZ97WiD_8P",
        "colab_type": "code",
        "outputId": "916e86ba-759d-4066-9d42-ef89ac37aa77",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        }
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard\n",
        "from keras.optimizers import SGD, Adam, RMSprop\n",
        "from keras.layers.merge import concatenate\n",
        "import matplotlib.pyplot as plt\n",
        "import keras.backend as K\n",
        "import tensorflow as tf\n",
        "import imgaug as ia\n",
        "from tqdm import tqdm\n",
        "from imgaug import augmenters as iaa\n",
        "import numpy as np\n",
        "import pickle\n",
        "import os, cv2\n",
        "from preprocessing import parse_annotation, BatchGenerator\n",
        "from utils import WeightReader, decode_netout, draw_boxes\n",
        "\n",
        "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dW1FSzluEGuK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LABELS = ['Apple','Banana','Broccoli','Cabbage','Carrot','Grape','Mango','Orange','Pear','Pineapple','Potato','Radish','Squash','Strawberry','Watermelon','Zucchini']\n",
        "\n",
        "IMAGE_H, IMAGE_W = 416, 416\n",
        "GRID_H,  GRID_W  = 13 , 13\n",
        "BOX              = 5\n",
        "CLASS            = len(LABELS)\n",
        "CLASS_WEIGHTS    = np.ones(CLASS, dtype='float32')\n",
        "OBJ_THRESHOLD    = 0.3#0.5\n",
        "NMS_THRESHOLD    = 0.3#0.45\n",
        "ANCHORS          = [0.57273, 0.677385, 1.87446, 2.06253, 3.33843, 5.47434, 7.88282, 3.52778, 9.77052, 9.16828]\n",
        "\n",
        "NO_OBJECT_SCALE  = 1.0\n",
        "OBJECT_SCALE     = 5.0\n",
        "COORD_SCALE      = 1.0\n",
        "CLASS_SCALE      = 1.0\n",
        "\n",
        "BATCH_SIZE       = 16\n",
        "WARM_UP_BATCHES  = 0\n",
        "TRUE_BOX_BUFFER  = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IGxizsj7EZ3i",
        "colab_type": "code",
        "outputId": "7a7d614f-8a12-4d3c-9a4b-b843092defbd",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 75
        }
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-3dde80eb-73c1-44da-9323-be0f1059eb10\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-3dde80eb-73c1-44da-9323-be0f1059eb10\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving yolov2.weights to yolov2.weights\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdGvrItEEULq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wt_path = 'yolov2.weights'                      \n",
        "train_image_folder = 'train_images/'\n",
        "train_annot_folder = 'train_annots/'\n",
        "valid_image_folder = 'val_images/'\n",
        "valid_annot_folder = 'val_annots/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RNw6fGaOH3a4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def space_to_depth_x2(x):\n",
        "    return tf.space_to_depth(x, block_size=2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4HIVtOCCH6aA",
        "colab_type": "code",
        "outputId": "151f2c34-7483-4460-9cf4-df38a9d7b063",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        }
      },
      "source": [
        "input_image = Input(shape=(IMAGE_H, IMAGE_W, 3))\n",
        "true_boxes  = Input(shape=(1, 1, 1, TRUE_BOX_BUFFER , 4))\n",
        "\n",
        "# Layer 1\n",
        "x = Conv2D(32, (3,3), strides=(1,1), padding='same', name='conv_1', use_bias=False)(input_image)\n",
        "x = BatchNormalization(name='norm_1')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "# Layer 2\n",
        "x = Conv2D(64, (3,3), strides=(1,1), padding='same', name='conv_2', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_2')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "# Layer 3\n",
        "x = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_3', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_3')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 4\n",
        "x = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_4', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_4')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 5\n",
        "x = Conv2D(128, (3,3), strides=(1,1), padding='same', name='conv_5', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_5')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "# Layer 6\n",
        "x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_6', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_6')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 7\n",
        "x = Conv2D(128, (1,1), strides=(1,1), padding='same', name='conv_7', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_7')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 8\n",
        "x = Conv2D(256, (3,3), strides=(1,1), padding='same', name='conv_8', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_8')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "# Layer 9\n",
        "x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_9', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_9')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 10\n",
        "x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_10', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_10')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 11\n",
        "x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_11', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_11')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 12\n",
        "x = Conv2D(256, (1,1), strides=(1,1), padding='same', name='conv_12', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_12')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 13\n",
        "x = Conv2D(512, (3,3), strides=(1,1), padding='same', name='conv_13', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_13')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "skip_connection = x\n",
        "\n",
        "x = MaxPooling2D(pool_size=(2, 2))(x)\n",
        "\n",
        "# Layer 14\n",
        "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_14', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_14')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 15\n",
        "x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_15', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_15')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 16\n",
        "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_16', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_16')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 17\n",
        "x = Conv2D(512, (1,1), strides=(1,1), padding='same', name='conv_17', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_17')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 18\n",
        "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_18', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_18')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 19\n",
        "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_19', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_19')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 20\n",
        "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_20', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_20')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 21\n",
        "skip_connection = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_21', use_bias=False)(skip_connection)\n",
        "skip_connection = BatchNormalization(name='norm_21')(skip_connection)\n",
        "skip_connection = LeakyReLU(alpha=0.1)(skip_connection)\n",
        "skip_connection = Lambda(space_to_depth_x2)(skip_connection)\n",
        "\n",
        "x = concatenate([skip_connection, x])\n",
        "\n",
        "# Layer 22\n",
        "x = Conv2D(1024, (3,3), strides=(1,1), padding='same', name='conv_22', use_bias=False)(x)\n",
        "x = BatchNormalization(name='norm_22')(x)\n",
        "x = LeakyReLU(alpha=0.1)(x)\n",
        "\n",
        "# Layer 23\n",
        "x = Conv2D(BOX * (4 + 1 + CLASS), (1,1), strides=(1,1), padding='same', name='conv_23')(x)\n",
        "output = Reshape((GRID_H, GRID_W, BOX, 4 + 1 + CLASS))(x)\n",
        "\n",
        "# small hack to allow true_boxes to be registered when Keras build the model \n",
        "# for more information: https://github.com/fchollet/keras/issues/2790\n",
        "output = Lambda(lambda args: args[0])([output, true_boxes])\n",
        "\n",
        "model = Model([input_image, true_boxes], output)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qyyi4EhqH8nV",
        "colab_type": "code",
        "outputId": "90fdce8f-d2b0-479d-b7e4-b96dc955f54b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 416, 416, 3)  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv_1 (Conv2D)                 (None, 416, 416, 32) 864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_1 (BatchNormalization)     (None, 416, 416, 32) 128         conv_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 416, 416, 32) 0           norm_1[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 208, 208, 32) 0           leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_2 (Conv2D)                 (None, 208, 208, 64) 18432       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "norm_2 (BatchNormalization)     (None, 208, 208, 64) 256         conv_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 208, 208, 64) 0           norm_2[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 104, 104, 64) 0           leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_3 (Conv2D)                 (None, 104, 104, 128 73728       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "norm_3 (BatchNormalization)     (None, 104, 104, 128 512         conv_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 104, 104, 128 0           norm_3[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv_4 (Conv2D)                 (None, 104, 104, 64) 8192        leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "norm_4 (BatchNormalization)     (None, 104, 104, 64) 256         conv_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 104, 104, 64) 0           norm_4[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv_5 (Conv2D)                 (None, 104, 104, 128 73728       leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "norm_5 (BatchNormalization)     (None, 104, 104, 128 512         conv_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, 104, 104, 128 0           norm_5[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 52, 52, 128)  0           leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_6 (Conv2D)                 (None, 52, 52, 256)  294912      max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "norm_6 (BatchNormalization)     (None, 52, 52, 256)  1024        conv_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, 52, 52, 256)  0           norm_6[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv_7 (Conv2D)                 (None, 52, 52, 128)  32768       leaky_re_lu_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "norm_7 (BatchNormalization)     (None, 52, 52, 128)  512         conv_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, 52, 52, 128)  0           norm_7[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv_8 (Conv2D)                 (None, 52, 52, 256)  294912      leaky_re_lu_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "norm_8 (BatchNormalization)     (None, 52, 52, 256)  1024        conv_8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 52, 52, 256)  0           norm_8[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 26, 26, 256)  0           leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv_9 (Conv2D)                 (None, 26, 26, 512)  1179648     max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "norm_9 (BatchNormalization)     (None, 26, 26, 512)  2048        conv_9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 26, 26, 512)  0           norm_9[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "conv_10 (Conv2D)                (None, 26, 26, 256)  131072      leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "norm_10 (BatchNormalization)    (None, 26, 26, 256)  1024        conv_10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, 26, 26, 256)  0           norm_10[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_11 (Conv2D)                (None, 26, 26, 512)  1179648     leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_11 (BatchNormalization)    (None, 26, 26, 512)  2048        conv_11[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, 26, 26, 512)  0           norm_11[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_12 (Conv2D)                (None, 26, 26, 256)  131072      leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_12 (BatchNormalization)    (None, 26, 26, 256)  1024        conv_12[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)      (None, 26, 26, 256)  0           norm_12[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_13 (Conv2D)                (None, 26, 26, 512)  1179648     leaky_re_lu_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_13 (BatchNormalization)    (None, 26, 26, 512)  2048        conv_13[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)      (None, 26, 26, 512)  0           norm_13[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_5 (MaxPooling2D)  (None, 13, 13, 512)  0           leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv_14 (Conv2D)                (None, 13, 13, 1024) 4718592     max_pooling2d_5[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "norm_14 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_14[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_14[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_15 (Conv2D)                (None, 13, 13, 512)  524288      leaky_re_lu_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_15 (BatchNormalization)    (None, 13, 13, 512)  2048        conv_15[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)      (None, 13, 13, 512)  0           norm_15[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_16 (Conv2D)                (None, 13, 13, 1024) 4718592     leaky_re_lu_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_16 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_16[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_16[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_17 (Conv2D)                (None, 13, 13, 512)  524288      leaky_re_lu_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_17 (BatchNormalization)    (None, 13, 13, 512)  2048        conv_17[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)      (None, 13, 13, 512)  0           norm_17[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_18 (Conv2D)                (None, 13, 13, 1024) 4718592     leaky_re_lu_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_18 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_18[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_18 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_18[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_19 (Conv2D)                (None, 13, 13, 1024) 9437184     leaky_re_lu_18[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_19 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_19[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_21 (Conv2D)                (None, 26, 26, 64)   32768       leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_19 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_19[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_21 (BatchNormalization)    (None, 26, 26, 64)   256         conv_21[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_20 (Conv2D)                (None, 13, 13, 1024) 9437184     leaky_re_lu_19[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_21 (LeakyReLU)      (None, 26, 26, 64)   0           norm_21[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "norm_20 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_20[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 13, 13, 256)  0           leaky_re_lu_21[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_20 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_20[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 13, 13, 1280) 0           lambda_1[0][0]                   \n",
            "                                                                 leaky_re_lu_20[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv_22 (Conv2D)                (None, 13, 13, 1024) 11796480    concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "norm_22 (BatchNormalization)    (None, 13, 13, 1024) 4096        conv_22[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_22 (LeakyReLU)      (None, 13, 13, 1024) 0           norm_22[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv_23 (Conv2D)                (None, 13, 13, 105)  107625      leaky_re_lu_22[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "reshape_1 (Reshape)             (None, 13, 13, 5, 21 0           conv_23[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            (None, 1, 1, 1, 50,  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 13, 13, 5, 21 0           reshape_1[0][0]                  \n",
            "                                                                 input_2[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 50,655,561\n",
            "Trainable params: 50,634,889\n",
            "Non-trainable params: 20,672\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_THl06M5H-2X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight_reader = WeightReader(wt_path)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aq00OCFMIC-A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "weight_reader.reset()\n",
        "nb_conv = 23\n",
        "\n",
        "for i in range(1, nb_conv+1):\n",
        "    conv_layer = model.get_layer('conv_' + str(i))\n",
        "    \n",
        "    if i < nb_conv:\n",
        "        norm_layer = model.get_layer('norm_' + str(i))\n",
        "        \n",
        "        size = np.prod(norm_layer.get_weights()[0].shape)\n",
        "\n",
        "        beta  = weight_reader.read_bytes(size)\n",
        "        gamma = weight_reader.read_bytes(size)\n",
        "        mean  = weight_reader.read_bytes(size)\n",
        "        var   = weight_reader.read_bytes(size)\n",
        "\n",
        "        weights = norm_layer.set_weights([gamma, beta, mean, var])       \n",
        "        \n",
        "    if len(conv_layer.get_weights()) > 1:\n",
        "        bias   = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[1].shape))\n",
        "        kernel = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
        "        kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
        "        kernel = kernel.transpose([2,3,1,0])\n",
        "        conv_layer.set_weights([kernel, bias])\n",
        "    else:\n",
        "        kernel = weight_reader.read_bytes(np.prod(conv_layer.get_weights()[0].shape))\n",
        "        kernel = kernel.reshape(list(reversed(conv_layer.get_weights()[0].shape)))\n",
        "        kernel = kernel.transpose([2,3,1,0])\n",
        "        conv_layer.set_weights([kernel])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9J3FyODWIDyu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layer   = model.layers[-4] # the last convolutional layer\n",
        "weights = layer.get_weights()\n",
        "\n",
        "new_kernel = np.random.normal(size=weights[0].shape)/(GRID_H*GRID_W)\n",
        "new_bias   = np.random.normal(size=weights[1].shape)/(GRID_H*GRID_W)\n",
        "\n",
        "layer.set_weights([new_kernel, new_bias])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOiQ3mPzIGJI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def custom_loss(y_true, y_pred):\n",
        "    mask_shape = tf.shape(y_true)[:4]\n",
        "    \n",
        "    cell_x = tf.to_float(tf.reshape(tf.tile(tf.range(GRID_W), [GRID_H]), (1, GRID_H, GRID_W, 1, 1)))\n",
        "    cell_y = tf.transpose(cell_x, (0,2,1,3,4))\n",
        "\n",
        "    cell_grid = tf.tile(tf.concat([cell_x,cell_y], -1), [BATCH_SIZE, 1, 1, 5, 1])\n",
        "    \n",
        "    coord_mask = tf.zeros(mask_shape)\n",
        "    conf_mask  = tf.zeros(mask_shape)\n",
        "    class_mask = tf.zeros(mask_shape)\n",
        "    \n",
        "    seen = tf.Variable(0.)\n",
        "    total_recall = tf.Variable(0.)\n",
        "    \n",
        "    \"\"\"\n",
        "    Adjust prediction\n",
        "    \"\"\"\n",
        "    ### adjust x and y      \n",
        "    pred_box_xy = tf.sigmoid(y_pred[..., :2]) + cell_grid\n",
        "    \n",
        "    ### adjust w and h\n",
        "    pred_box_wh = tf.exp(y_pred[..., 2:4]) * np.reshape(ANCHORS, [1,1,1,BOX,2])\n",
        "    \n",
        "    ### adjust confidence\n",
        "    pred_box_conf = tf.sigmoid(y_pred[..., 4])\n",
        "    \n",
        "    ### adjust class probabilities\n",
        "    pred_box_class = y_pred[..., 5:]\n",
        "    \n",
        "    \"\"\"\n",
        "    Adjust ground truth\n",
        "    \"\"\"\n",
        "    ### adjust x and y\n",
        "    true_box_xy = y_true[..., 0:2] # relative position to the containing cell\n",
        "    \n",
        "    ### adjust w and h\n",
        "    true_box_wh = y_true[..., 2:4] # number of cells accross, horizontally and vertically\n",
        "    \n",
        "    ### adjust confidence\n",
        "    true_wh_half = true_box_wh / 2.\n",
        "    true_mins    = true_box_xy - true_wh_half\n",
        "    true_maxes   = true_box_xy + true_wh_half\n",
        "    \n",
        "    pred_wh_half = pred_box_wh / 2.\n",
        "    pred_mins    = pred_box_xy - pred_wh_half\n",
        "    pred_maxes   = pred_box_xy + pred_wh_half       \n",
        "    \n",
        "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
        "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
        "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
        "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
        "    \n",
        "    true_areas = true_box_wh[..., 0] * true_box_wh[..., 1]\n",
        "    pred_areas = pred_box_wh[..., 0] * pred_box_wh[..., 1]\n",
        "\n",
        "    union_areas = pred_areas + true_areas - intersect_areas\n",
        "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
        "    \n",
        "    true_box_conf = iou_scores * y_true[..., 4]\n",
        "    \n",
        "    ### adjust class probabilities\n",
        "    true_box_class = tf.argmax(y_true[..., 5:], -1)\n",
        "    \n",
        "    \"\"\"\n",
        "    Determine the masks\n",
        "    \"\"\"\n",
        "    ### coordinate mask: simply the position of the ground truth boxes (the predictors)\n",
        "    coord_mask = tf.expand_dims(y_true[..., 4], axis=-1) * COORD_SCALE\n",
        "    \n",
        "    ### confidence mask: penelize predictors + penalize boxes with low IOU\n",
        "    # penalize the confidence of the boxes, which have IOU with some ground truth box < 0.6\n",
        "    true_xy = true_boxes[..., 0:2]\n",
        "    true_wh = true_boxes[..., 2:4]\n",
        "    \n",
        "    true_wh_half = true_wh / 2.\n",
        "    true_mins    = true_xy - true_wh_half\n",
        "    true_maxes   = true_xy + true_wh_half\n",
        "    \n",
        "    pred_xy = tf.expand_dims(pred_box_xy, 4)\n",
        "    pred_wh = tf.expand_dims(pred_box_wh, 4)\n",
        "    \n",
        "    pred_wh_half = pred_wh / 2.\n",
        "    pred_mins    = pred_xy - pred_wh_half\n",
        "    pred_maxes   = pred_xy + pred_wh_half    \n",
        "    \n",
        "    intersect_mins  = tf.maximum(pred_mins,  true_mins)\n",
        "    intersect_maxes = tf.minimum(pred_maxes, true_maxes)\n",
        "    intersect_wh    = tf.maximum(intersect_maxes - intersect_mins, 0.)\n",
        "    intersect_areas = intersect_wh[..., 0] * intersect_wh[..., 1]\n",
        "    \n",
        "    true_areas = true_wh[..., 0] * true_wh[..., 1]\n",
        "    pred_areas = pred_wh[..., 0] * pred_wh[..., 1]\n",
        "\n",
        "    union_areas = pred_areas + true_areas - intersect_areas\n",
        "    iou_scores  = tf.truediv(intersect_areas, union_areas)\n",
        "\n",
        "    best_ious = tf.reduce_max(iou_scores, axis=4)\n",
        "    conf_mask = conf_mask + tf.to_float(best_ious < 0.6) * (1 - y_true[..., 4]) * NO_OBJECT_SCALE\n",
        "    \n",
        "    # penalize the confidence of the boxes, which are reponsible for corresponding ground truth box\n",
        "    conf_mask = conf_mask + y_true[..., 4] * OBJECT_SCALE\n",
        "    \n",
        "    ### class mask: simply the position of the ground truth boxes (the predictors)\n",
        "    class_mask = y_true[..., 4] * tf.gather(CLASS_WEIGHTS, true_box_class) * CLASS_SCALE       \n",
        "    \n",
        "    \"\"\"\n",
        "    Warm-up training\n",
        "    \"\"\"\n",
        "    no_boxes_mask = tf.to_float(coord_mask < COORD_SCALE/2.)\n",
        "    seen = tf.assign_add(seen, 1.)\n",
        "    \n",
        "    true_box_xy, true_box_wh, coord_mask = tf.cond(tf.less(seen, WARM_UP_BATCHES), \n",
        "                          lambda: [true_box_xy + (0.5 + cell_grid) * no_boxes_mask, \n",
        "                                   true_box_wh + tf.ones_like(true_box_wh) * np.reshape(ANCHORS, [1,1,1,BOX,2]) * no_boxes_mask, \n",
        "                                   tf.ones_like(coord_mask)],\n",
        "                          lambda: [true_box_xy, \n",
        "                                   true_box_wh,\n",
        "                                   coord_mask])\n",
        "    \n",
        "    \"\"\"\n",
        "    Finalize the loss\n",
        "    \"\"\"\n",
        "    nb_coord_box = tf.reduce_sum(tf.to_float(coord_mask > 0.0))\n",
        "    nb_conf_box  = tf.reduce_sum(tf.to_float(conf_mask  > 0.0))\n",
        "    nb_class_box = tf.reduce_sum(tf.to_float(class_mask > 0.0))\n",
        "    \n",
        "    loss_xy    = tf.reduce_sum(tf.square(true_box_xy-pred_box_xy)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
        "    loss_wh    = tf.reduce_sum(tf.square(true_box_wh-pred_box_wh)     * coord_mask) / (nb_coord_box + 1e-6) / 2.\n",
        "    loss_conf  = tf.reduce_sum(tf.square(true_box_conf-pred_box_conf) * conf_mask)  / (nb_conf_box  + 1e-6) / 2.\n",
        "    loss_class = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=true_box_class, logits=pred_box_class)\n",
        "    loss_class = tf.reduce_sum(loss_class * class_mask) / (nb_class_box + 1e-6)\n",
        "    \n",
        "    loss = loss_xy + loss_wh + loss_conf + loss_class\n",
        "    \n",
        "    nb_true_box = tf.reduce_sum(y_true[..., 4])\n",
        "    nb_pred_box = tf.reduce_sum(tf.to_float(true_box_conf > 0.5) * tf.to_float(pred_box_conf > 0.3))\n",
        "\n",
        "    \"\"\"\n",
        "    Debugging code\n",
        "    \"\"\"    \n",
        "    current_recall = nb_pred_box/(nb_true_box + 1e-6)\n",
        "    total_recall = tf.assign_add(total_recall, current_recall) \n",
        "\n",
        "    loss = tf.Print(loss, [tf.zeros((1))], message='Dummy Line \\t', summarize=1000)\n",
        "    loss = tf.Print(loss, [loss_xy], message='Loss XY \\t', summarize=1000)\n",
        "    loss = tf.Print(loss, [loss_wh], message='Loss WH \\t', summarize=1000)\n",
        "    loss = tf.Print(loss, [loss_conf], message='Loss Conf \\t', summarize=1000)\n",
        "    loss = tf.Print(loss, [loss_class], message='Loss Class \\t', summarize=1000)\n",
        "    loss = tf.Print(loss, [loss], message='Total Loss \\t', summarize=1000)\n",
        "    loss = tf.Print(loss, [current_recall], message='Current Recall \\t', summarize=1000)\n",
        "    loss = tf.Print(loss, [total_recall/seen], message='Average Recall \\t', summarize=1000)\n",
        "    \n",
        "    return loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nj_DBh37IKdi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "generator_config = {\n",
        "    'IMAGE_H'         : IMAGE_H, \n",
        "    'IMAGE_W'         : IMAGE_W,\n",
        "    'GRID_H'          : GRID_H,  \n",
        "    'GRID_W'          : GRID_W,\n",
        "    'BOX'             : BOX,\n",
        "    'LABELS'          : LABELS,\n",
        "    'CLASS'           : len(LABELS),\n",
        "    'ANCHORS'         : ANCHORS,\n",
        "    'BATCH_SIZE'      : BATCH_SIZE,\n",
        "    'TRUE_BOX_BUFFER' : 50,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y4ChnPpQIMw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def normalize(image):\n",
        "    return image / 255."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o6rGyrdoINkd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_imgs, seen_train_labels = parse_annotation(train_annot_folder, train_image_folder, labels=LABELS)\n",
        "### write parsed annotations to pickle for fast retrieval next time\n",
        "#with open('train_imgs', 'wb') as fp:\n",
        "#    pickle.dump(train_imgs, fp)\n",
        "\n",
        "### read saved pickle of parsed annotations\n",
        "#with open ('train_imgs', 'rb') as fp:\n",
        "#    train_imgs = pickle.load(fp)\n",
        "train_batch = BatchGenerator(train_imgs, generator_config, norm=normalize)\n",
        "\n",
        "valid_imgs, seen_valid_labels = parse_annotation(valid_annot_folder, valid_image_folder, labels=LABELS)\n",
        "### write parsed annotations to pickle for fast retrieval next time\n",
        "#with open('valid_imgs', 'wb') as fp:\n",
        "#    pickle.dump(valid_imgs, fp)\n",
        "\n",
        "### read saved pickle of parsed annotations\n",
        "#with open ('valid_imgs', 'rb') as fp:\n",
        "#    valid_imgs = pickle.load(fp)\n",
        "valid_batch = BatchGenerator(valid_imgs, generator_config, norm=normalize, jitter=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQim5abGIR4l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "early_stop = EarlyStopping(monitor='val_loss', \n",
        "                           min_delta=0.001, \n",
        "                           patience=3, \n",
        "                           mode='min', \n",
        "                           verbose=1)\n",
        "\n",
        "checkpoint = ModelCheckpoint('weights_coco.h5', \n",
        "                             monitor='val_loss', \n",
        "                             verbose=1, \n",
        "                             save_best_only=True, \n",
        "                             mode='min', \n",
        "                             period=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Zif7VFaIUYE",
        "colab_type": "code",
        "outputId": "4db1b8ba-e52a-4753-a0a7-fbd9fa2e0457",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 952
        }
      },
      "source": [
        "\n",
        "optimizer = Adam(lr=0.5e-4, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)\n",
        "#optimizer = SGD(lr=1e-4, decay=0.0005, momentum=0.9)\n",
        "#optimizer = RMSprop(lr=1e-4, rho=0.9, epsilon=1e-08, decay=0.0)\n",
        "\n",
        "model.compile(loss=custom_loss, optimizer=optimizer)\n",
        "\n",
        "model.fit_generator(generator        = train_batch, \n",
        "                    steps_per_epoch  = len(train_batch), \n",
        "                    epochs           = 15, \n",
        "                    verbose          = 1,\n",
        "                    validation_data  = valid_batch,\n",
        "                    validation_steps = len(valid_batch),\n",
        "                    callbacks        = [early_stop, checkpoint])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-121-1c1e9c0c2ea6>:4: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.cast` instead.\n",
            "WARNING:tensorflow:From <ipython-input-121-1c1e9c0c2ea6>:145: Print (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2018-08-20.\n",
            "Instructions for updating:\n",
            "Use tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/15\n",
            "78/78 [==============================] - 3722s 48s/step - loss: 4.1461 - val_loss: 4.0804\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 4.08042, saving model to weights_coco.h5\n",
            "Epoch 2/15\n",
            "78/78 [==============================] - 3698s 47s/step - loss: 3.3469 - val_loss: 3.4967\n",
            "\n",
            "Epoch 00002: val_loss improved from 4.08042 to 3.49667, saving model to weights_coco.h5\n",
            "Epoch 3/15\n",
            "78/78 [==============================] - 3692s 47s/step - loss: 2.9491 - val_loss: 3.0791\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.49667 to 3.07906, saving model to weights_coco.h5\n",
            "Epoch 4/15\n",
            "78/78 [==============================] - 3431s 44s/step - loss: 2.6919 - val_loss: 2.9146\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.07906 to 2.91465, saving model to weights_coco.h5\n",
            "Epoch 5/15\n",
            "78/78 [==============================] - 3356s 43s/step - loss: 2.4235 - val_loss: 2.7958\n",
            "\n",
            "Epoch 00005: val_loss improved from 2.91465 to 2.79583, saving model to weights_coco.h5\n",
            "Epoch 6/15\n",
            "78/78 [==============================] - 3404s 44s/step - loss: 2.1819 - val_loss: 2.5973\n",
            "\n",
            "Epoch 00006: val_loss improved from 2.79583 to 2.59732, saving model to weights_coco.h5\n",
            "Epoch 7/15\n",
            "78/78 [==============================] - 3558s 46s/step - loss: 1.9389 - val_loss: 2.3619\n",
            "\n",
            "Epoch 00007: val_loss improved from 2.59732 to 2.36185, saving model to weights_coco.h5\n",
            "Epoch 8/15\n",
            "78/78 [==============================] - 3430s 44s/step - loss: 1.7836 - val_loss: 2.4356\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 2.36185\n",
            "Epoch 9/15\n",
            "78/78 [==============================] - 3645s 47s/step - loss: 1.6108 - val_loss: 2.3014\n",
            "\n",
            "Epoch 00009: val_loss improved from 2.36185 to 2.30142, saving model to weights_coco.h5\n",
            "Epoch 10/15\n",
            "30/78 [==========>...................] - ETA: 33:45 - loss: 1.4485"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}